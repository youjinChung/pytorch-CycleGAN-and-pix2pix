{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_model_export.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHoMIXmaTkA",
        "colab_type": "text"
      },
      "source": [
        "# How to export with code example\n",
        "\n",
        "In general, the procedure for model export is pretty straightforward thanks to good integration of onnx in Pytorch.\n",
        "\n",
        "The code itself is simple. First we import torch and build a test model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e8b58npaI8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XoIN-DBaqQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Conv2d(in_channels=3, out_channels=1, \n",
        "                               kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer(x)\n",
        "        out = nn.functional.interpolate(out, scale_factor=2, \n",
        "                                        mode='bilinear', align_corners=True)\n",
        "        out = torch.nn.functional.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp2dTgv5apT-",
        "colab_type": "text"
      },
      "source": [
        "The code for the export itself is then"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plo9l9LJaoNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "model.eval()\n",
        "random_input = torch.randn(1, 3, 64, 64, dtype=torch.float32)\n",
        "# you can add however many inputs your model or task requires\n",
        "\n",
        "input_names = [\"image\"]\n",
        "output_names = [\"pred\"]\n",
        "\n",
        "torch.onnx.export(model, random_input, './model.onnx', verbose=False, \n",
        "                  input_names=input_names, output_names=output_names, \n",
        "                  opset_version=11)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbxeCuziblon",
        "colab_type": "text"
      },
      "source": [
        "It is important to make sure that the number of elements in `input_names` is the same as the number of input arguments in your model’s `forward` method. As well as that the number of return variables of `forward` method is the same as the number of elements in `output_names`.\n",
        "\n",
        "Make sure that input shapes are correct since you won’t be able to change them when you import the model in LensStudio. It just takes them from your .onnx file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJJUZkAcAZ7",
        "colab_type": "text"
      },
      "source": [
        "# How to load the model into LensStudio and what import errors you can get\n",
        "\n",
        "The process for importing your model into LensStudio is again straightforward. You just need to add an ML component and it will prompt you to select a file containing your model. Select your ONNX file that you’ve exported previously and if everything is fine, the studio will prompt you to set your model’s input and output scale and bias. \n",
        "\n",
        "In the upper left corner of this prompt you will see a compatibility table of Ops in your model and different available inference frameworks. These are used to accelerate model inference if the device that’s running your lens has the ability to utilize these frameworks and if all of the Ops are implemented in a given inference framework. If your model has unsupported Ops, you can hover over the warning icon and it will provide you with a layer name that has this issue.\n",
        "\n",
        "![](https://storage.googleapis.com/snapchat-lens-assets/f1a09194-f02d-43ed-92b8-62e843179ff0/lensStudio/Guides/kfmqObLkNr_3_0/image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCglqIETcjuc",
        "colab_type": "text"
      },
      "source": [
        "One of the most useful apps for debugging ONNX files is https://github.com/lutzroeder/netron.\n",
        "You can open your .onnx file in Netron and find the Op that’s causing the issue by name.\n",
        "\n",
        "![alt text](https://storage.googleapis.com/snapchat-lens-assets/f1a09194-f02d-43ed-92b8-62e843179ff0/lensStudio/Guides/kfmqObLkNr_3_0/image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figVwarnc7Pm",
        "colab_type": "text"
      },
      "source": [
        "You can then see if you can reimplement your model avoiding unsupported operations.\n",
        "\n",
        "Remember that LensStudio textures have values in the range of [0, 255] so if your mode was trained with whitened input with values in [-1, 1] you need to correctly set scale and bias parameters to reflect that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyg-ko7dAUM",
        "colab_type": "text"
      },
      "source": [
        "# Common issues with PyTorch -> ONNX conversion\n",
        "\n",
        "* One of the major issues is bilinear interpolation. There is a discrepancy \n",
        "between Pytorch and mobile inference frameworks in handling edges of interpolated image with `align_corners` set to `False`. So you need to make sure your model uses `align_corners=True` everywhere it uses bilinear interpolation. Also you should use `opset_version=11` if you have `align_corners=True` in your model, since default 9th opset doesn’t have this parameter in its op definition. \n",
        "It might also be better to use nearest neighbor interpolation or transposed convolution instead of bilinear interpolation, since iOS inference accelerator doesn’t support `align_corners=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFJ6WSbxheo6",
        "colab_type": "code",
        "outputId": "2c90d054-1337-426a-d161-f6796eb08192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "x = random_input\n",
        "# bad\n",
        "nn.functional.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "\n",
        "# better\n",
        "nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "# best\n",
        "nn.ConvTranspose2d(3, 3, 3)\n",
        "nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "nn.Upsample(scale_factor=2, mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Upsample(scale_factor=2.0, mode=nearest)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1H3WbCxhfoy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* Pay attention to your `.view()` and `.reshape()` operations in your network. Main issue with those is that while Pytorch uses NCHW format for it’s 4 dimensional tensors, different mobile inference frameworks can use either NCHW or NHWC format. If possible replace them with operations that preserve tensor dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLg2zfVhgbh",
        "colab_type": "code",
        "outputId": "cb699d78-a2de-4144-ba4f-a4bf80f93ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# bad: NCHW dim order is assumed\n",
        "features = features.view(batch_size, num_channels, height, width)\n",
        "result = conv(features)\n",
        "# ok: dim order is irrelevant\n",
        "flattened_features = features.view(batch_size, -1)\n",
        "result = flattened_features.sum(dim=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8114, -1.2428,  0.3517,  ...,  0.9097, -0.3542, -0.0816]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX42yheSkF88",
        "colab_type": "text"
      },
      "source": [
        "* Some Pytorch versions have issues with different ONNX opsets. If you are encountering issues exporting model with interpolation, softmax layer with set dim parameter, try to update your Pytorch to the latest available version and set `opset_version=11` parameter in your `torch.onnx.export` function call."
      ]
    }
  ]
}